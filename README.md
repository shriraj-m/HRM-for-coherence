# HRM-for-coherence
Altering the default Hierarchical Reasoning Model (HRM) to apply for Coherence / QA Tasks.

# ğŸ“š Overview

This project, developed by the NLP Foreigners team for CAI4304, explores how the Hierarchical Reasoning Model (HRM). A neuroscience-inspired architecture, can be adapted to Natural Language Processing (NLP) tasks such as question answering (QA) and coherence reasoning.
Our main goal is to extend the HRM architecture with an attention feedback mechanism to enable multi-hop reasoning and context refinement within a two-module framework.


# ğŸ‘¥ Team Members
Shriraj Mandulapalli: Project Lead
Designed project scope, oversaw architecture development, implemented attention feedback between modules.

Matthieu Drouin: Data Specialist
Conducted dataset curation and literature review on HRM and NLP datasets.

InÃ©s Alonso: Low-Level Module Researcher
Led literature review and design for token-level module.

Alec Brenes: High-Level Module Researcher	
Focused on architecture and reasoning flow in the high-level module.


# ğŸ¯ Problem Statement

Current transformer-based NLP models excel at pattern matching but struggle with deep reasoning, compositional generalization, and multi-hop question answering.
The Hierarchical Reasoning Model (HRM) introduces a recurrent two-module structure inspired by neuroscience, but it has never been applied to NLP.

Our project aims to:

-Adapt HRM for NLP reasoning and coherence tasks.

-Introduce attention feedback between high-level and low-level modules.

-Demonstrate hierarchical multi-hop reasoning without massive datasets.


# ğŸ“Š Data Sets

TBD

# ğŸ“ Project Structure

TBD


# ğŸ§¾ References

Wang et al., Hierarchical Reasoning Model, (2025).

Bounsi et al., Transformers Meet Neural Algorithmic Reasoners, (2024).

Wei & Tay et al., Chain-of-Thought Prompting Elicits Reasoning in LLMs, (2022).

Gong & Bowman, Ruminating Reader: Reasoning with Gated Multi-Hop Attention, ACL (2018).

Guo & Chen, Decoupling Knowledge and Reasoning in Transformers, (2025).
